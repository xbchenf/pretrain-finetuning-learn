{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizer 基本使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 学术资源加速\n",
    "import subprocess\n",
    "import os\n",
    "\n",
    "result = subprocess.run('bash -c \"source /etc/network_turbo && env | grep proxy\"', shell=True, capture_output=True, text=True)\n",
    "output = result.stdout\n",
    "for line in output.splitlines():\n",
    "    if '=' in line:\n",
    "        var, value = line.split('=', 1)\n",
    "        os.environ[var] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sen = \"吃葡萄不吐葡萄皮!\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step1 加载与保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertTokenizerFast(name_or_path='uer/roberta-base-finetuned-dianping-chinese', vocab_size=21128, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 从HuggingFace加载，输入模型名称，即可加载对于的分词器\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"uer/roberta-base-finetuned-dianping-chinese\")\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./roberta_tokenizer/tokenizer_config.json',\n",
       " './roberta_tokenizer/special_tokens_map.json',\n",
       " './roberta_tokenizer/vocab.txt',\n",
       " './roberta_tokenizer/added_tokens.json',\n",
       " './roberta_tokenizer/tokenizer.json')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenizer 保存到本地\n",
    "tokenizer.save_pretrained(\"./roberta_tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertTokenizerFast(name_or_path='./roberta_tokenizer/', vocab_size=21128, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 从本地加载tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./roberta_tokenizer/\")\n",
    "tokenizer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step2 句子分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['吃', '葡', '萄', '不', '吐', '葡', '萄', '皮', '!']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = tokenizer.tokenize(sen)\n",
    "tokens"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step3 查看词典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'##頑': 20579,\n",
       " '##餘': 20683,\n",
       " '楝': 3505,\n",
       " '─': 427,\n",
       " 'ed2k': 10587,\n",
       " '##燿': 17311,\n",
       " '##醫': 20072,\n",
       " '##洵': 16886,\n",
       " '捐': 2935,\n",
       " 'fedora': 11577,\n",
       " '་': 286,\n",
       " '##car': 9831,\n",
       " 'oracle': 10085,\n",
       " 'nand': 11711,\n",
       " '##剑': 14244,\n",
       " '##奠': 15007,\n",
       " '##洩': 16881,\n",
       " '钎': 7154,\n",
       " '欽': 3620,\n",
       " '##砗': 17836,\n",
       " '131': 9403,\n",
       " '##聞': 18529,\n",
       " '##跋': 19705,\n",
       " '##lax': 12680,\n",
       " '麩': 7932,\n",
       " '##凝': 14182,\n",
       " '密': 2166,\n",
       " 'coach': 10784,\n",
       " '441': 13013,\n",
       " '羿': 5418,\n",
       " '##鑒': 20199,\n",
       " '3p': 12108,\n",
       " '##編': 18283,\n",
       " 'lz': 10767,\n",
       " '##斗': 16216,\n",
       " '俄': 915,\n",
       " '睐': 4712,\n",
       " '緩': 5227,\n",
       " '賠': 6543,\n",
       " '##彷': 15570,\n",
       " '##攢': 16168,\n",
       " '構': 3539,\n",
       " '舀': 5641,\n",
       " '##ment': 8631,\n",
       " '##兢': 14113,\n",
       " '##la': 8461,\n",
       " '##潋': 17103,\n",
       " '##咄': 14523,\n",
       " '树': 3409,\n",
       " '##oc': 9450,\n",
       " 'fks': 10044,\n",
       " '##写': 14148,\n",
       " '##號': 19055,\n",
       " '##钴': 20237,\n",
       " '##改': 16178,\n",
       " '峇': 2280,\n",
       " '##俺': 13996,\n",
       " 'ultra': 10893,\n",
       " 'ふ': 564,\n",
       " '蝌': 6069,\n",
       " '剃': 1178,\n",
       " '##bi': 9350,\n",
       " '##帆': 15416,\n",
       " '81': 8424,\n",
       " '##騨': 20758,\n",
       " '##薩': 19015,\n",
       " '##统': 18377,\n",
       " '##銃': 20123,\n",
       " '##妩': 15039,\n",
       " '鉀': 7054,\n",
       " '##蟑': 19153,\n",
       " '蒻': 5894,\n",
       " '##摁': 16085,\n",
       " '##鄢': 20027,\n",
       " '##dia': 9360,\n",
       " '##sk': 8998,\n",
       " 'event': 10992,\n",
       " '##猩': 17399,\n",
       " '##磊': 17887,\n",
       " '婚': 2042,\n",
       " '##暨': 16327,\n",
       " '##稽': 17999,\n",
       " '##莧': 18866,\n",
       " 'xxx': 8790,\n",
       " '拆': 2858,\n",
       " '##剪': 14255,\n",
       " '##洮': 16883,\n",
       " '××': 8858,\n",
       " '裡': 6174,\n",
       " '咂': 1465,\n",
       " '##zz': 9409,\n",
       " 'emi': 13104,\n",
       " '氲': 3716,\n",
       " '1984': 8655,\n",
       " '##沒': 16817,\n",
       " '##ｄ': 9835,\n",
       " '##暑': 16321,\n",
       " '##演': 17085,\n",
       " '岐': 2262,\n",
       " '差': 2345,\n",
       " 'ﾟ': 8099,\n",
       " 'research': 10018,\n",
       " '##￣': 21122,\n",
       " '瀝': 4110,\n",
       " '謝': 6342,\n",
       " '逊': 6849,\n",
       " '##挫': 15976,\n",
       " '##age': 9103,\n",
       " '烁': 4161,\n",
       " '##奶': 15016,\n",
       " '##啫': 14628,\n",
       " '##簫': 18141,\n",
       " '湫': 3964,\n",
       " '##查': 16446,\n",
       " '##put': 11300,\n",
       " '}': 171,\n",
       " '琛': 4422,\n",
       " '偌': 972,\n",
       " '膀': 5598,\n",
       " '1974': 9072,\n",
       " '##喙': 14654,\n",
       " '##策': 18089,\n",
       " '覆': 6208,\n",
       " '荼': 5795,\n",
       " '##錮': 20153,\n",
       " '莊': 5800,\n",
       " 'nikon': 11391,\n",
       " '##杠': 16396,\n",
       " '稠': 4932,\n",
       " '##涓': 16928,\n",
       " '刑': 1152,\n",
       " 'σ': 226,\n",
       " '##誼': 19366,\n",
       " '統': 5186,\n",
       " '嘧': 1665,\n",
       " '埼': 1826,\n",
       " '##91': 9332,\n",
       " '##喊': 14648,\n",
       " '嘘': 1656,\n",
       " '甬': 4503,\n",
       " '##鼹': 21021,\n",
       " '##end': 11652,\n",
       " '碎': 4810,\n",
       " 'ai': 8578,\n",
       " '##啮': 14630,\n",
       " '巿': 2354,\n",
       " '芙': 5696,\n",
       " '311': 10256,\n",
       " '##bo': 8820,\n",
       " '##呜': 14506,\n",
       " '7500': 11271,\n",
       " '哋': 1508,\n",
       " 'adidas': 8953,\n",
       " '沥': 3769,\n",
       " '勋': 1237,\n",
       " '##髖': 20826,\n",
       " '♫': 494,\n",
       " '雾': 7443,\n",
       " '##堕': 14891,\n",
       " '##kk': 12693,\n",
       " '捎': 2933,\n",
       " '閔': 7280,\n",
       " '蟾': 6103,\n",
       " '闌': 7296,\n",
       " '##is': 8331,\n",
       " '##rl': 9923,\n",
       " 'x5': 10871,\n",
       " '彎': 2494,\n",
       " 'des': 11081,\n",
       " 'お': 540,\n",
       " '拥': 2881,\n",
       " '瓷': 4487,\n",
       " '窠': 4975,\n",
       " '泪': 3801,\n",
       " '﹕': 8004,\n",
       " '##03': 9042,\n",
       " 'unit': 12816,\n",
       " '##ⓒ': 13575,\n",
       " '捏': 2934,\n",
       " '冕': 1089,\n",
       " '撤': 3059,\n",
       " '408': 12189,\n",
       " '##価': 13957,\n",
       " '488': 12789,\n",
       " '##農': 19860,\n",
       " '##ads': 12514,\n",
       " '##晁': 16285,\n",
       " '##阉': 20386,\n",
       " '##鬣': 20840,\n",
       " '阖': 7335,\n",
       " '##湮': 17022,\n",
       " '泳': 3807,\n",
       " 'eba': 11129,\n",
       " '##ument': 11867,\n",
       " '復': 2541,\n",
       " '沾': 3783,\n",
       " 'learning': 12315,\n",
       " 'god': 11138,\n",
       " '##駆': 20744,\n",
       " '囲': 1739,\n",
       " '##ather': 12290,\n",
       " '##乩': 13799,\n",
       " '##龕': 21043,\n",
       " '>': 135,\n",
       " '##灰': 17186,\n",
       " '##叻': 14444,\n",
       " '##rid': 10804,\n",
       " '##荠': 18838,\n",
       " '510': 10907,\n",
       " '##food': 12973,\n",
       " '##纨': 18335,\n",
       " '##蓿': 18968,\n",
       " 'ん': 585,\n",
       " '##ley': 8930,\n",
       " '溟': 3979,\n",
       " '257': 11027,\n",
       " '哎': 1511,\n",
       " '8mm': 12964,\n",
       " '調': 6310,\n",
       " '橢': 3584,\n",
       " '##xxx': 12812,\n",
       " '##eg': 10935,\n",
       " '琬': 4428,\n",
       " '5m': 11483,\n",
       " 'ㄒ': 656,\n",
       " '##仝': 13861,\n",
       " '布': 2357,\n",
       " '##蟾': 19160,\n",
       " '##蹈': 19745,\n",
       " '開': 7274,\n",
       " '##立': 18046,\n",
       " 'pixnetfacebookyahoo': 12047,\n",
       " '##form': 12725,\n",
       " '##旖': 16242,\n",
       " '浦': 3855,\n",
       " '##孙': 15158,\n",
       " '伢': 838,\n",
       " '890': 12075,\n",
       " '蠢': 6111,\n",
       " 'information': 11237,\n",
       " '##傾': 14062,\n",
       " '粽': 5124,\n",
       " '##唐': 14595,\n",
       " '##扑': 15857,\n",
       " '痫': 4587,\n",
       " '葭': 5874,\n",
       " 'pro': 8376,\n",
       " '1860': 11990,\n",
       " '##易': 16268,\n",
       " '##瑚': 17502,\n",
       " '鼾': 7966,\n",
       " '##in': 8277,\n",
       " 'planet': 12269,\n",
       " 'c5': 12648,\n",
       " '##圧': 14818,\n",
       " '##浣': 16911,\n",
       " '##窯': 18039,\n",
       " 'final': 10591,\n",
       " 'shopping': 9662,\n",
       " '##撞': 16115,\n",
       " '##澹': 17136,\n",
       " '錢': 7092,\n",
       " '烊': 4165,\n",
       " '瓊': 4475,\n",
       " '综': 5341,\n",
       " '##tra': 9808,\n",
       " '2004': 8258,\n",
       " '[unused42]': 42,\n",
       " 'ц': 254,\n",
       " '##傻': 14061,\n",
       " '兲': 1067,\n",
       " 'alex': 10179,\n",
       " '##啓': 14616,\n",
       " '##褔': 19246,\n",
       " '##嚷': 14771,\n",
       " '≥': 395,\n",
       " '##磋': 17888,\n",
       " 'amana': 12406,\n",
       " '##飯': 20670,\n",
       " '穂': 4945,\n",
       " '唔': 1540,\n",
       " '##08': 9153,\n",
       " '製': 6182,\n",
       " '娠': 2027,\n",
       " '##ʰ': 13370,\n",
       " 'overchicstoretvhome': 12450,\n",
       " '##珲': 17467,\n",
       " '1963': 9155,\n",
       " '薄': 5946,\n",
       " '##傚': 14049,\n",
       " '##彆': 15548,\n",
       " '偕': 975,\n",
       " '##紜': 18218,\n",
       " '##鱔': 20877,\n",
       " 'ㄏ': 655,\n",
       " '##tty': 11160,\n",
       " '镂': 7251,\n",
       " '啤': 1566,\n",
       " '匝': 1268,\n",
       " 'ａ': 8051,\n",
       " 'อ': 282,\n",
       " '小': 2207,\n",
       " '48': 8214,\n",
       " '##氦': 16765,\n",
       " '##眩': 17757,\n",
       " '##桦': 16499,\n",
       " '##悸': 15711,\n",
       " '耒': 5448,\n",
       " '##簾': 18145,\n",
       " '##嶙': 15383,\n",
       " '犢': 4303,\n",
       " '##湘': 17017,\n",
       " 'forest': 10889,\n",
       " '乓': 729,\n",
       " '廝': 2447,\n",
       " '##嵬': 15377,\n",
       " '##鲱': 20894,\n",
       " '俎': 917,\n",
       " '16': 8121,\n",
       " '##rris': 11818,\n",
       " '##κ': 13388,\n",
       " 'ｔ': 8070,\n",
       " '##貳': 19579,\n",
       " '诵': 6433,\n",
       " '徕': 2532,\n",
       " '##bor': 12268,\n",
       " '##ふ': 13671,\n",
       " '[unused76]': 76,\n",
       " '翎': 5423,\n",
       " '襪': 6201,\n",
       " '##竇': 18044,\n",
       " '℃': 360,\n",
       " 'target': 11926,\n",
       " '##2016': 8386,\n",
       " '鐳': 7135,\n",
       " 'starbucks': 12919,\n",
       " '##墘': 14926,\n",
       " '抽': 2853,\n",
       " '##緞': 18280,\n",
       " '溧': 3982,\n",
       " '##bike': 13276,\n",
       " '荨': 5787,\n",
       " '##イン': 10108,\n",
       " '駱': 7694,\n",
       " '##｝': 21102,\n",
       " '##毒': 16738,\n",
       " '##驰': 20777,\n",
       " '鸳': 7892,\n",
       " '##传': 13894,\n",
       " '##ッセーシを': 11231,\n",
       " '##叩': 14428,\n",
       " '鞦': 7497,\n",
       " '轮': 6762,\n",
       " 'wed': 8585,\n",
       " 'pure': 13179,\n",
       " '021': 10385,\n",
       " '承': 2824,\n",
       " '置': 5390,\n",
       " '##儀': 14078,\n",
       " '##癜': 17677,\n",
       " '##gy': 9765,\n",
       " 'e': 147,\n",
       " '尔': 2209,\n",
       " '饥': 7645,\n",
       " 'eric': 9836,\n",
       " '纔': 5269,\n",
       " 'application': 12339,\n",
       " '##遜': 19950,\n",
       " '##轶': 19824,\n",
       " 'howard': 11312,\n",
       " '辛': 6789,\n",
       " 'イカせるテンマ': 10885,\n",
       " '臭': 5634,\n",
       " '##痹': 17648,\n",
       " '##柱': 16450,\n",
       " '按': 2902,\n",
       " '##瀕': 17163,\n",
       " '##殷': 16725,\n",
       " '謙': 6340,\n",
       " '##癇': 17673,\n",
       " 'qq': 8186,\n",
       " '##铁': 20245,\n",
       " '饶': 7657,\n",
       " '990': 10278,\n",
       " '##当': 15553,\n",
       " '披': 2847,\n",
       " '##淙': 16963,\n",
       " '酰': 6995,\n",
       " 'close': 11239,\n",
       " '##峥': 15343,\n",
       " '芩': 5702,\n",
       " '##俎': 13974,\n",
       " '##www': 9718,\n",
       " '##ろ': 10460,\n",
       " '监': 4664,\n",
       " '账': 6572,\n",
       " '##嗲': 14697,\n",
       " '##菀': 18877,\n",
       " '##讧': 19430,\n",
       " '塘': 1851,\n",
       " '泌': 3789,\n",
       " '##肋': 18547,\n",
       " '萘': 5849,\n",
       " '##武': 16693,\n",
       " '##琲': 17488,\n",
       " '##釐': 20088,\n",
       " '1111': 11954,\n",
       " '畿': 4537,\n",
       " '1000': 8212,\n",
       " '朋': 3301,\n",
       " '泱': 3806,\n",
       " 'ا': 261,\n",
       " '爹': 4269,\n",
       " '蟄': 6092,\n",
       " 'idc': 9617,\n",
       " '嗑': 1622,\n",
       " 'zip': 10947,\n",
       " '##碌': 17865,\n",
       " '萃': 5842,\n",
       " '##蚊': 19068,\n",
       " '楹': 3516,\n",
       " '仨': 810,\n",
       " '戕': 2771,\n",
       " '##膳': 18669,\n",
       " 'dac': 12913,\n",
       " '烷': 4180,\n",
       " '蚣': 6017,\n",
       " '##陟': 20423,\n",
       " '##酌': 20037,\n",
       " '##锣': 20293,\n",
       " 'green': 9631,\n",
       " '期': 3309,\n",
       " '##塢': 14911,\n",
       " 'h1': 12333,\n",
       " '⦿': 509,\n",
       " '吻': 1431,\n",
       " '螺': 6090,\n",
       " '餌': 7622,\n",
       " '##扒': 15858,\n",
       " 'glass': 12535,\n",
       " '袂': 6146,\n",
       " '欄': 3608,\n",
       " '##baby': 12515,\n",
       " '呜': 1449,\n",
       " '请': 6435,\n",
       " '拯': 2889,\n",
       " '郴': 6959,\n",
       " '睥': 4720,\n",
       " '鬃': 7776,\n",
       " '璽': 4473,\n",
       " 'ra': 12619,\n",
       " '##蹼': 19763,\n",
       " '尻': 2224,\n",
       " '##嚴': 14770,\n",
       " '##體': 20825,\n",
       " '##挙': 15963,\n",
       " '##ud': 9849,\n",
       " '##腫': 18641,\n",
       " '猫': 4344,\n",
       " '##喀': 14641,\n",
       " '##イト': 10516,\n",
       " '##商': 14612,\n",
       " '##骈': 20795,\n",
       " '馒': 7672,\n",
       " '舎': 5651,\n",
       " 'qqmei': 12011,\n",
       " '##urg': 13143,\n",
       " '##橙': 16638,\n",
       " '##鸽': 20951,\n",
       " '##住': 13914,\n",
       " '##ist': 9527,\n",
       " '〇': 514,\n",
       " '舸': 5669,\n",
       " '##tory': 12608,\n",
       " '##粳': 18177,\n",
       " '##蜀': 19100,\n",
       " '盾': 4688,\n",
       " '##绮': 18388,\n",
       " '##lo': 8897,\n",
       " 'そんな': 11950,\n",
       " '##ground': 13195,\n",
       " '##hl': 11831,\n",
       " '砲': 4787,\n",
       " '貌': 6505,\n",
       " '々': 513,\n",
       " '##踮': 19737,\n",
       " '算': 5050,\n",
       " '##佢': 13930,\n",
       " '##32': 8709,\n",
       " '整': 3146,\n",
       " 'article': 9122,\n",
       " '##盂': 17712,\n",
       " '##讶': 19442,\n",
       " '哐': 1513,\n",
       " '##埵': 14878,\n",
       " '##虫': 19058,\n",
       " '淫': 3915,\n",
       " '撻': 3071,\n",
       " 'etc': 9813,\n",
       " '灭': 4127,\n",
       " '慚': 2712,\n",
       " '##涪': 16944,\n",
       " '熙': 4224,\n",
       " '麋': 7924,\n",
       " '##潟': 17111,\n",
       " '肿': 5514,\n",
       " '##rity': 12079,\n",
       " '##‹': 13506,\n",
       " '##洒': 16875,\n",
       " '壬': 1895,\n",
       " '##ak': 9896,\n",
       " '##璟': 17526,\n",
       " '##陶': 20435,\n",
       " '##匙': 14324,\n",
       " '##rk': 10364,\n",
       " 'て': 555,\n",
       " '傭': 999,\n",
       " '導': 2206,\n",
       " '搓': 3013,\n",
       " '钼': 7184,\n",
       " 'javascript': 8561,\n",
       " '##议': 19436,\n",
       " '##豎': 19549,\n",
       " '##n1': 11310,\n",
       " '##謔': 19395,\n",
       " 'here': 10815,\n",
       " '##黔': 21005,\n",
       " '廓': 2444,\n",
       " '##黝': 21009,\n",
       " '##徬': 15600,\n",
       " '##茎': 18806,\n",
       " '##ller': 11284,\n",
       " '##俑': 13977,\n",
       " '2b': 10740,\n",
       " '娘': 2023,\n",
       " '##ㄤ': 13720,\n",
       " '##呕': 14502,\n",
       " '##鐳': 20192,\n",
       " '燙': 4243,\n",
       " 'school': 9467,\n",
       " '1944': 9462,\n",
       " 'left': 12744,\n",
       " '饲': 7654,\n",
       " '##昊': 16264,\n",
       " '揚': 2993,\n",
       " '懑': 2749,\n",
       " '293': 11855,\n",
       " '巴': 2349,\n",
       " '##hia': 12615,\n",
       " '##冷': 14164,\n",
       " '##澤': 17132,\n",
       " '##痉': 17627,\n",
       " '鈦': 7049,\n",
       " '##益': 17717,\n",
       " '障': 7397,\n",
       " '##剔': 14245,\n",
       " '##蟄': 19149,\n",
       " '箐': 5047,\n",
       " '##搅': 16066,\n",
       " '##案': 16485,\n",
       " 'linkedin': 11369,\n",
       " '娑': 2021,\n",
       " '孵': 2118,\n",
       " '崇': 2300,\n",
       " '##喫': 14661,\n",
       " ';': 132,\n",
       " '+': 116,\n",
       " '噜': 1686,\n",
       " '醴': 7019,\n",
       " '##tt': 9235,\n",
       " '锑': 7230,\n",
       " '##hur': 13190,\n",
       " '##淀': 16952,\n",
       " 'mvp': 9505,\n",
       " '１２': 10351,\n",
       " '##肱': 18565,\n",
       " '##测': 16901,\n",
       " '糙': 5133,\n",
       " '狄': 4313,\n",
       " '##遊': 19936,\n",
       " '镉': 7253,\n",
       " '蛤': 6035,\n",
       " '##けて': 12214,\n",
       " '剷': 1202,\n",
       " '挡': 2913,\n",
       " '##site': 11479,\n",
       " '##∽': 13541,\n",
       " '鳗': 7849,\n",
       " '酗': 6984,\n",
       " 'may': 8480,\n",
       " '90': 8192,\n",
       " '贝': 6564,\n",
       " '軾': 6732,\n",
       " '尊': 2203,\n",
       " '酮': 6993,\n",
       " '岳': 2277,\n",
       " '饱': 7653,\n",
       " '勞': 1246,\n",
       " '鲛': 7830,\n",
       " '噹': 1698,\n",
       " 'amoled': 11307,\n",
       " 'bwl': 9681,\n",
       " 'costco': 10742,\n",
       " '##添': 16981,\n",
       " '##託': 19306,\n",
       " '##蹊': 19747,\n",
       " '[unused4]': 4,\n",
       " '│': 429,\n",
       " '##す': 8764,\n",
       " '##怠': 15648,\n",
       " '##豢': 19554,\n",
       " '##秭': 17972,\n",
       " '##〔': 13661,\n",
       " '0': 121,\n",
       " '##カー': 10175,\n",
       " '253': 10221,\n",
       " 'g1': 11528,\n",
       " '##蹦': 19755,\n",
       " '璉': 4463,\n",
       " '宥': 2148,\n",
       " '酝': 6986,\n",
       " '武': 3636,\n",
       " '##co': 8588,\n",
       " '##弁': 15516,\n",
       " '钜': 7161,\n",
       " '[unused59]': 59,\n",
       " '钣': 7168,\n",
       " '铤': 7204,\n",
       " '迂': 6811,\n",
       " '442': 13171,\n",
       " '##扉': 15853,\n",
       " '##腿': 18654,\n",
       " '鯉': 7805,\n",
       " '##受': 14415,\n",
       " '##廖': 15502,\n",
       " '##搭': 16079,\n",
       " '匮': 1273,\n",
       " '##ム': 9227,\n",
       " '##羔': 18459,\n",
       " '実': 2142,\n",
       " '##苋': 18777,\n",
       " '##醮': 20074,\n",
       " '詆': 6265,\n",
       " '##颡': 20643,\n",
       " '場': 1842,\n",
       " '##･': 21107,\n",
       " '异': 2460,\n",
       " '##ow': 9024,\n",
       " '##砾': 17850,\n",
       " '恤': 2614,\n",
       " '犁': 4298,\n",
       " '卧': 1309,\n",
       " 'ᅯ': 316,\n",
       " '滇': 3995,\n",
       " '17': 8126,\n",
       " '##ますのて': 13071,\n",
       " '##ue': 8803,\n",
       " '##,': 13328,\n",
       " '##槓': 16601,\n",
       " '##碰': 17878,\n",
       " '##赅': 19656,\n",
       " '##逵': 19926,\n",
       " '├': 436,\n",
       " '抑': 2829,\n",
       " '陋': 7358,\n",
       " '贵': 6586,\n",
       " 'august': 9217,\n",
       " 'cba': 10912,\n",
       " 'com™': 10921,\n",
       " '鸥': 7886,\n",
       " 'は１ヶ': 11638,\n",
       " '##粹': 18179,\n",
       " 'form': 12244,\n",
       " '檳': 3599,\n",
       " 'chris': 9582,\n",
       " 'ㄉ': 650,\n",
       " 'george': 9897,\n",
       " 'deep': 12592,\n",
       " '##荘': 18834,\n",
       " '##嘉': 14706,\n",
       " '嘀': 1645,\n",
       " '##ru': 10409,\n",
       " 'ღ': 287,\n",
       " '阅': 7325,\n",
       " '磚': 4834,\n",
       " 'wx17house': 9973,\n",
       " '擱': 3095,\n",
       " '覃': 6207,\n",
       " '##れている': 11856,\n",
       " '鹹': 7919,\n",
       " '##麼': 20995,\n",
       " '##⑩': 13565,\n",
       " '##諮': 19381,\n",
       " 'am09': 12377,\n",
       " '音': 7509,\n",
       " '##棂': 16524,\n",
       " '輾': 6747,\n",
       " '婕': 2041,\n",
       " '嶙': 2326,\n",
       " '惱': 2681,\n",
       " '##秘': 17965,\n",
       " '##紋': 18208,\n",
       " '##拽': 15952,\n",
       " '##獷': 17422,\n",
       " '织': 5302,\n",
       " '##ㄋ': 13709,\n",
       " '##▋': 13602,\n",
       " '媞': 2057,\n",
       " '嘔': 1653,\n",
       " '形': 2501,\n",
       " '擴': 3097,\n",
       " '購': 6554,\n",
       " '##硒': 17854,\n",
       " '78': 8409,\n",
       " '##浯': 16917,\n",
       " '##蕊': 18991,\n",
       " '##赐': 19663,\n",
       " '便': 912,\n",
       " '##颍': 20629,\n",
       " '##艺': 18743,\n",
       " 'ssd': 9004,\n",
       " '##苓': 18782,\n",
       " '浸': 3863,\n",
       " '##衄': 19175,\n",
       " '##阆': 20383,\n",
       " '##labels': 10636,\n",
       " '國': 1751,\n",
       " '##淫': 16972,\n",
       " '鐘': 7132,\n",
       " '愍': 2691,\n",
       " '．': 8026,\n",
       " '柯': 3392,\n",
       " '铢': 7202,\n",
       " 'steam': 9325,\n",
       " 'mb': 9499,\n",
       " '374': 11948,\n",
       " '谩': 6475,\n",
       " '飩': 7611,\n",
       " '荊': 5771,\n",
       " '连': 6825,\n",
       " 'kai': 13072,\n",
       " '罢': 5387,\n",
       " '##沮': 16832,\n",
       " '##痂': 17623,\n",
       " '##〖': 13663,\n",
       " '##册': 14142,\n",
       " '冗': 1090,\n",
       " '庾': 2437,\n",
       " '玫': 4382,\n",
       " '鳄': 7843,\n",
       " '##ury': 11117,\n",
       " '戬': 2780,\n",
       " '247': 11128,\n",
       " '##挟': 15968,\n",
       " 'word': 8681,\n",
       " '凹': 1138,\n",
       " '##µ': 13355,\n",
       " '##ｂ': 12641,\n",
       " '燼': 4253,\n",
       " '##內': 14115,\n",
       " '烂': 4162,\n",
       " 'ユ': 632,\n",
       " '♪': 493,\n",
       " '粿': 5126,\n",
       " '西': 6205,\n",
       " '##⊙': 13551,\n",
       " '##琦': 17482,\n",
       " '##蹟': 19752,\n",
       " '##闽': 20377,\n",
       " '##600': 10346,\n",
       " '##吒': 14461,\n",
       " '諡': 6319,\n",
       " '##盎': 17718,\n",
       " 'pg': 12898,\n",
       " '脏': 5552,\n",
       " '減': 3938,\n",
       " '##int': 9824,\n",
       " '菌': 5826,\n",
       " '马': 7716,\n",
       " '##js': 11124,\n",
       " '梢': 3456,\n",
       " '和': 1469,\n",
       " '劑': 1212,\n",
       " '绎': 5306,\n",
       " '##fi': 9864,\n",
       " '镗': 7260,\n",
       " '5k': 12172,\n",
       " 'arpg': 12701,\n",
       " '秣': 4910,\n",
       " '攢': 3111,\n",
       " '##ย': 13446,\n",
       " '##塩': 14912,\n",
       " '##43': 9433,\n",
       " '##癱': 17686,\n",
       " '##†': 13498,\n",
       " '##超': 19688,\n",
       " '##棵': 16541,\n",
       " '##狀': 17368,\n",
       " '倡': 956,\n",
       " 'hu': 12199,\n",
       " '壇': 1883,\n",
       " '##嘍': 14708,\n",
       " '牦': 4287,\n",
       " 'before': 10735,\n",
       " '繫': 5258,\n",
       " '##餐': 20680,\n",
       " '弱': 2483,\n",
       " '##のてすか': 13221,\n",
       " '##ano': 10855,\n",
       " 'pc': 8295,\n",
       " '躺': 6720,\n",
       " '##凉': 14174,\n",
       " '##茱': 18816,\n",
       " '##利': 14221,\n",
       " '包': 1259,\n",
       " '运': 6817,\n",
       " '鍊': 7101,\n",
       " '##栄': 16458,\n",
       " '##咦': 14541,\n",
       " '##tta': 11245,\n",
       " '僻': 1020,\n",
       " '[unused25]': 25,\n",
       " '煊': 4201,\n",
       " '吵': 1427,\n",
       " '##╰': 13594,\n",
       " 'ヘ': 622,\n",
       " '##︱': 21050,\n",
       " '178': 9649,\n",
       " '##威': 15071,\n",
       " '##衫': 19193,\n",
       " '箫': 5054,\n",
       " '薰': 5962,\n",
       " '##汽': 16806,\n",
       " '##酸': 20057,\n",
       " '##参': 14403,\n",
       " 'anthony': 10219,\n",
       " '##碛': 17873,\n",
       " '##钗': 20215,\n",
       " '8g': 10019,\n",
       " '蔼': 5928,\n",
       " 'exhibition': 13282,\n",
       " '##ement': 11946,\n",
       " '盒': 4665,\n",
       " '##旮': 16253,\n",
       " '##lling': 12903,\n",
       " '##疖': 17601,\n",
       " '##瘴': 17669,\n",
       " '##祈': 17914,\n",
       " '##μ': 13390,\n",
       " '##矿': 17828,\n",
       " '而': 5445,\n",
       " '##镭': 20321,\n",
       " '##马': 20773,\n",
       " '睫': 4724,\n",
       " '磡': 4835,\n",
       " '##徽': 15608,\n",
       " '訝': 6252,\n",
       " '##漁': 17079,\n",
       " '1970': 8464,\n",
       " '##娅': 15074,\n",
       " '##幸': 15458,\n",
       " '沒': 3760,\n",
       " '##厳': 14399,\n",
       " '诈': 6400,\n",
       " '##店': 15478,\n",
       " '##桨': 16501,\n",
       " '锰': 7243,\n",
       " '##矫': 17820,\n",
       " '315': 9613,\n",
       " 'cpa': 11963,\n",
       " '粘': 5111,\n",
       " '獲': 4363,\n",
       " '##ち': 9967,\n",
       " '##塗': 14907,\n",
       " '##姪': 15065,\n",
       " '##矗': 17812,\n",
       " '椎': 3491,\n",
       " '1987': 8622,\n",
       " '滅': 3994,\n",
       " '轄': 6749,\n",
       " '1969': 9073,\n",
       " '摧': 3039,\n",
       " '##諡': 19376,\n",
       " '乒': 728,\n",
       " '滟': 4006,\n",
       " '##●': 9037,\n",
       " '窑': 4965,\n",
       " '绩': 5327,\n",
       " '##驿': 20788,\n",
       " '傑': 989,\n",
       " '頑': 7522,\n",
       " '効': 1230,\n",
       " '##ies': 8773,\n",
       " '##简': 18099,\n",
       " '##～3': 12617,\n",
       " '塩': 1855,\n",
       " '##腌': 18631,\n",
       " '##嘚': 14714,\n",
       " '600': 8298,\n",
       " 've': 12810,\n",
       " '廚': 2446,\n",
       " '##冨': 14155,\n",
       " '##退': 19899,\n",
       " '倘': 951,\n",
       " '##22': 8683,\n",
       " '##倚': 14010,\n",
       " '##嘴': 14730,\n",
       " '璇': 4462,\n",
       " '船': 5670,\n",
       " '##賜': 19598,\n",
       " '##臃': 18677,\n",
       " '颗': 7578,\n",
       " '##尽': 15283,\n",
       " '月': 3299,\n",
       " '逼': 6873,\n",
       " 'game': 9033,\n",
       " '蟑': 6096,\n",
       " '##疥': 17610,\n",
       " '##迥': 19886,\n",
       " '##勻': 14312,\n",
       " 'charles': 10403,\n",
       " '壟': 1890,\n",
       " '##琨': 17483,\n",
       " '##狄': 17370,\n",
       " 'してくたさい': 10650,\n",
       " '呕': 1445,\n",
       " '##潛': 17108,\n",
       " '貢': 6513,\n",
       " 'oo': 11383,\n",
       " '##烦': 17229,\n",
       " '绕': 5312,\n",
       " '##ᄇ': 13460,\n",
       " '骄': 7734,\n",
       " 'twice': 13054,\n",
       " '吁': 1390,\n",
       " 'fresh': 12718,\n",
       " '##rip': 12332,\n",
       " '1951': 9387,\n",
       " '翔': 5425,\n",
       " '##此': 16691,\n",
       " '##稼': 17998,\n",
       " '##须': 20614,\n",
       " 'suite': 11420,\n",
       " '##殼': 16727,\n",
       " '##讲': 19439,\n",
       " '##ku': 9547,\n",
       " '##看': 17749,\n",
       " '愁': 2687,\n",
       " '2025': 8950,\n",
       " '##岙': 15325,\n",
       " '##涡': 16937,\n",
       " 'international': 9001,\n",
       " '##沙': 16820,\n",
       " '##驕': 20766,\n",
       " '##リー': 10535,\n",
       " '票': 4873,\n",
       " '##猜': 17396,\n",
       " 'posts': 10639,\n",
       " '##unch': 11294,\n",
       " '焗': 4187,\n",
       " '##侣': 13958,\n",
       " 'とは': 11556,\n",
       " '冶': 1106,\n",
       " '咸': 1496,\n",
       " '##44': 9292,\n",
       " 'ᅩ': 311,\n",
       " '丝': 692,\n",
       " '0t': 10691,\n",
       " '##¤': 13349,\n",
       " '蠡': 6110,\n",
       " '##200': 9902,\n",
       " '##崂': 15355,\n",
       " '##曰': 16345,\n",
       " '182': 9952,\n",
       " '##贰': 19639,\n",
       " '##馥': 20734,\n",
       " '##鳩': 20910,\n",
       " '撬': 3063,\n",
       " '愆': 2688,\n",
       " '玺': 4389,\n",
       " '黒': 7947,\n",
       " 'audio': 11698,\n",
       " '##腰': 18644,\n",
       " '##诟': 19472,\n",
       " '##雀': 20468,\n",
       " 'ⓘ': 426,\n",
       " '##卦': 14365,\n",
       " '貓': 6506,\n",
       " '1929': 9792,\n",
       " '岙': 2268,\n",
       " '炊': 4141,\n",
       " '[unused55]': 55,\n",
       " 'n1': 11356,\n",
       " '肠': 5499,\n",
       " 'てある': 12190,\n",
       " '##嫣': 15130,\n",
       " '##潇': 17102,\n",
       " '憋': 2728,\n",
       " '羊': 5399,\n",
       " '##酪': 20048,\n",
       " ...}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21128"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab_size"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step4 索引转换"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1391, 5868, 5843, 679, 1402, 5868, 5843, 4649, 106]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 将词序列转换为id序列\n",
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['吃', '葡', '萄', '不', '吐', '葡', '萄', '皮', '!']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 将id序列转换为token序列\n",
    "tokens = tokenizer.convert_ids_to_tokens(ids)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'吃 葡 萄 不 吐 葡 萄 皮!'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 将token序列转换为string\n",
    "str_sen = tokenizer.convert_tokens_to_string(tokens)\n",
    "str_sen"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  更便捷的实现方式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 1391, 5868, 5843, 679, 1402, 5868, 5843, 4649, 106, 102]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 将字符串转换为id序列，又称之为编码\n",
    "ids = tokenizer.encode(sen, add_special_tokens=True)\n",
    "ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] 吃 葡 萄 不 吐 葡 萄 皮! [SEP]'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 将id序列转换为字符串，又称之为解码\n",
    "str_sen = tokenizer.decode(ids, skip_special_tokens=False)\n",
    "str_sen"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step5 填充与截断"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 1391, 5868, 5843, 679, 1402, 5868, 5843, 4649, 106, 102, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 填充\n",
    "ids = tokenizer.encode(sen, padding=\"max_length\", max_length=15)\n",
    "ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 1391, 5868, 5843, 102]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 截断\n",
    "ids = tokenizer.encode(sen, max_length=5, truncation=True)\n",
    "ids"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step6 其他输入部分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 1391, 5868, 5843, 679, 1402, 5868, 5843, 4649, 106, 102, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids = tokenizer.encode(sen, padding=\"max_length\", max_length=15)\n",
    "ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([101, 1391, 5868, 5843, 679, 1402, 5868, 5843, 4649, 106, 102, 0, 0, 0, 0],\n",
       " [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_mask = [1 if idx != 0 else 0 for idx in ids]\n",
    "token_type_ids = [0] * len(ids)\n",
    "ids, attention_mask, token_type_ids"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step7 快速调用方式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 1391, 5868, 5843, 679, 1402, 5868, 5843, 4649, 106, 102, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0]}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tokenizer.encode_plus(sen, padding=\"max_length\", max_length=15)\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 1391, 5868, 5843, 679, 1402, 5868, 5843, 4649, 106, 102, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0]}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tokenizer(sen, padding=\"max_length\", max_length=15)\n",
    "inputs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step8 处理batch数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[101, 1391, 5868, 5843, 679, 1402, 5868, 5843, 4649, 102], [101, 679, 1391, 5868, 5843, 1168, 1402, 5868, 5843, 4649, 102], [101, 7556, 1232, 5445, 711, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sens = [\"吃葡萄不吐葡萄皮\",\n",
    "        \"不吃葡萄到吐葡萄皮\",\n",
    "        \"顺势而为\"]\n",
    "res = tokenizer(sens)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 64.2 ms, sys: 0 ns, total: 64.2 ms\n",
      "Wall time: 62.7 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 单条循环处理\n",
    "for i in range(1000):\n",
    "    tokenizer(sen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 68.9 ms, sys: 20.5 ms, total: 89.4 ms\n",
      "Wall time: 13.2 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 处理batch数据\n",
    "res = tokenizer([sen] * 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertTokenizerFast(name_or_path='./roberta_tokenizer/', vocab_size=21128, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fast / Slow Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "sen = \"吃葡萄不吐葡萄皮!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertTokenizerFast(name_or_path='uer/roberta-base-finetuned-dianping-chinese', vocab_size=21128, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fast_tokenizer = AutoTokenizer.from_pretrained(\"uer/roberta-base-finetuned-dianping-chinese\")\n",
    "fast_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertTokenizer(name_or_path='uer/roberta-base-finetuned-dianping-chinese', vocab_size=21128, model_max_length=1000000000000000019884624838656, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slow_tokenizer = AutoTokenizer.from_pretrained(\"uer/roberta-base-finetuned-dianping-chinese\", use_fast=False)\n",
    "slow_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 78.1 ms\n",
      "Wall time: 363 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 单条循环处理\n",
    "for i in range(10000):\n",
    "    fast_tokenizer(sen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 188 ms\n",
      "Wall time: 877 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 单条循环处理\n",
    "for i in range(10000):\n",
    "    slow_tokenizer(sen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 266 ms\n",
      "Wall time: 80.6 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 处理batch数据\n",
    "res = fast_tokenizer([sen] * 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 219 ms\n",
      "Wall time: 738 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 处理batch数据\n",
    "res = slow_tokenizer([sen] * 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 2483, 2207, 4638, 2769, 738, 3300, 1920, 10252, 8221, 106, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'offset_mapping': [(0, 0), (0, 1), (1, 2), (2, 3), (3, 4), (4, 5), (5, 6), (6, 7), (7, 12), (12, 15), (15, 16), (0, 0)]}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = fast_tokenizer(sen, return_offsets_mapping=True)\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None, 0, 1, 2, 3, 4, 5, 6, 7, 7, 8, None]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.word_ids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "return_offset_mapping is not available when using Python tokenizers. To use this feature, change your tokenizer to one deriving from transformers.PreTrainedTokenizerFast. More information on available tokenizers at https://github.com/huggingface/transformers/pull/2674",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[44], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m inputs \u001b[39m=\u001b[39m slow_tokenizer(sen, return_offsets_mapping\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[1;32mc:\\Users\\yuyao\\miniconda3\\envs\\transformers\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2538\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.__call__\u001b[1;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m   2536\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_in_target_context_manager:\n\u001b[0;32m   2537\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_switch_to_input_mode()\n\u001b[1;32m-> 2538\u001b[0m     encodings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_one(text\u001b[39m=\u001b[39mtext, text_pair\u001b[39m=\u001b[39mtext_pair, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mall_kwargs)\n\u001b[0;32m   2539\u001b[0m \u001b[39mif\u001b[39;00m text_target \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   2540\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_switch_to_target_mode()\n",
      "File \u001b[1;32mc:\\Users\\yuyao\\miniconda3\\envs\\transformers\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2644\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._call_one\u001b[1;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m   2624\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_encode_plus(\n\u001b[0;32m   2625\u001b[0m         batch_text_or_text_pairs\u001b[39m=\u001b[39mbatch_text_or_text_pairs,\n\u001b[0;32m   2626\u001b[0m         add_special_tokens\u001b[39m=\u001b[39madd_special_tokens,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2641\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m   2642\u001b[0m     )\n\u001b[0;32m   2643\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 2644\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencode_plus(\n\u001b[0;32m   2645\u001b[0m         text\u001b[39m=\u001b[39mtext,\n\u001b[0;32m   2646\u001b[0m         text_pair\u001b[39m=\u001b[39mtext_pair,\n\u001b[0;32m   2647\u001b[0m         add_special_tokens\u001b[39m=\u001b[39madd_special_tokens,\n\u001b[0;32m   2648\u001b[0m         padding\u001b[39m=\u001b[39mpadding,\n\u001b[0;32m   2649\u001b[0m         truncation\u001b[39m=\u001b[39mtruncation,\n\u001b[0;32m   2650\u001b[0m         max_length\u001b[39m=\u001b[39mmax_length,\n\u001b[0;32m   2651\u001b[0m         stride\u001b[39m=\u001b[39mstride,\n\u001b[0;32m   2652\u001b[0m         is_split_into_words\u001b[39m=\u001b[39mis_split_into_words,\n\u001b[0;32m   2653\u001b[0m         pad_to_multiple_of\u001b[39m=\u001b[39mpad_to_multiple_of,\n\u001b[0;32m   2654\u001b[0m         return_tensors\u001b[39m=\u001b[39mreturn_tensors,\n\u001b[0;32m   2655\u001b[0m         return_token_type_ids\u001b[39m=\u001b[39mreturn_token_type_ids,\n\u001b[0;32m   2656\u001b[0m         return_attention_mask\u001b[39m=\u001b[39mreturn_attention_mask,\n\u001b[0;32m   2657\u001b[0m         return_overflowing_tokens\u001b[39m=\u001b[39mreturn_overflowing_tokens,\n\u001b[0;32m   2658\u001b[0m         return_special_tokens_mask\u001b[39m=\u001b[39mreturn_special_tokens_mask,\n\u001b[0;32m   2659\u001b[0m         return_offsets_mapping\u001b[39m=\u001b[39mreturn_offsets_mapping,\n\u001b[0;32m   2660\u001b[0m         return_length\u001b[39m=\u001b[39mreturn_length,\n\u001b[0;32m   2661\u001b[0m         verbose\u001b[39m=\u001b[39mverbose,\n\u001b[0;32m   2662\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m   2663\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\yuyao\\miniconda3\\envs\\transformers\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2717\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.encode_plus\u001b[1;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m   2707\u001b[0m \u001b[39m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[39;00m\n\u001b[0;32m   2708\u001b[0m padding_strategy, truncation_strategy, max_length, kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_padding_truncation_strategies(\n\u001b[0;32m   2709\u001b[0m     padding\u001b[39m=\u001b[39mpadding,\n\u001b[0;32m   2710\u001b[0m     truncation\u001b[39m=\u001b[39mtruncation,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2714\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m   2715\u001b[0m )\n\u001b[1;32m-> 2717\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_encode_plus(\n\u001b[0;32m   2718\u001b[0m     text\u001b[39m=\u001b[39mtext,\n\u001b[0;32m   2719\u001b[0m     text_pair\u001b[39m=\u001b[39mtext_pair,\n\u001b[0;32m   2720\u001b[0m     add_special_tokens\u001b[39m=\u001b[39madd_special_tokens,\n\u001b[0;32m   2721\u001b[0m     padding_strategy\u001b[39m=\u001b[39mpadding_strategy,\n\u001b[0;32m   2722\u001b[0m     truncation_strategy\u001b[39m=\u001b[39mtruncation_strategy,\n\u001b[0;32m   2723\u001b[0m     max_length\u001b[39m=\u001b[39mmax_length,\n\u001b[0;32m   2724\u001b[0m     stride\u001b[39m=\u001b[39mstride,\n\u001b[0;32m   2725\u001b[0m     is_split_into_words\u001b[39m=\u001b[39mis_split_into_words,\n\u001b[0;32m   2726\u001b[0m     pad_to_multiple_of\u001b[39m=\u001b[39mpad_to_multiple_of,\n\u001b[0;32m   2727\u001b[0m     return_tensors\u001b[39m=\u001b[39mreturn_tensors,\n\u001b[0;32m   2728\u001b[0m     return_token_type_ids\u001b[39m=\u001b[39mreturn_token_type_ids,\n\u001b[0;32m   2729\u001b[0m     return_attention_mask\u001b[39m=\u001b[39mreturn_attention_mask,\n\u001b[0;32m   2730\u001b[0m     return_overflowing_tokens\u001b[39m=\u001b[39mreturn_overflowing_tokens,\n\u001b[0;32m   2731\u001b[0m     return_special_tokens_mask\u001b[39m=\u001b[39mreturn_special_tokens_mask,\n\u001b[0;32m   2732\u001b[0m     return_offsets_mapping\u001b[39m=\u001b[39mreturn_offsets_mapping,\n\u001b[0;32m   2733\u001b[0m     return_length\u001b[39m=\u001b[39mreturn_length,\n\u001b[0;32m   2734\u001b[0m     verbose\u001b[39m=\u001b[39mverbose,\n\u001b[0;32m   2735\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m   2736\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\yuyao\\miniconda3\\envs\\transformers\\lib\\site-packages\\transformers\\tokenization_utils.py:641\u001b[0m, in \u001b[0;36mPreTrainedTokenizer._encode_plus\u001b[1;34m(self, text, text_pair, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m    635\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    636\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mInput \u001b[39m\u001b[39m{\u001b[39;00mtext\u001b[39m}\u001b[39;00m\u001b[39m is not valid. Should be a string, a list/tuple of strings or a list/tuple of\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    637\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m integers.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    638\u001b[0m             )\n\u001b[0;32m    640\u001b[0m \u001b[39mif\u001b[39;00m return_offsets_mapping:\n\u001b[1;32m--> 641\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m(\n\u001b[0;32m    642\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mreturn_offset_mapping is not available when using Python tokenizers. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    643\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mTo use this feature, change your tokenizer to one deriving from \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    644\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mtransformers.PreTrainedTokenizerFast. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    645\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mMore information on available tokenizers at \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    646\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mhttps://github.com/huggingface/transformers/pull/2674\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    647\u001b[0m     )\n\u001b[0;32m    649\u001b[0m first_ids \u001b[39m=\u001b[39m get_input_ids(text)\n\u001b[0;32m    650\u001b[0m second_ids \u001b[39m=\u001b[39m get_input_ids(text_pair) \u001b[39mif\u001b[39;00m text_pair \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[1;31mNotImplementedError\u001b[0m: return_offset_mapping is not available when using Python tokenizers. To use this feature, change your tokenizer to one deriving from transformers.PreTrainedTokenizerFast. More information on available tokenizers at https://github.com/huggingface/transformers/pull/2674"
     ]
    }
   ],
   "source": [
    "inputs = slow_tokenizer(sen, return_offsets_mapping=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 特殊Tokenizer的加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8357a6f05b4345baaffbb95f34fed2a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/441 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yuyao\\miniconda3\\envs\\transformers\\lib\\site-packages\\huggingface_hub\\file_download.py:133: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\yuyao\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ccfd3477c5c45e894350699ab1fffc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)enization_chatglm.py:   0%|          | 0.00/17.0k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e3ce5c1fcbb4d10b64c6b766cf99d95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading ice_text.model:   0%|          | 0.00/2.71M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "ChatGLMTokenizer(name_or_path='THUDM/chatglm-6b', vocab_size=130344, model_max_length=2048, is_fast=False, padding_side='left', truncation_side='right', special_tokens={'bos_token': '<sop>', 'eos_token': '<eop>', 'unk_token': '<unk>', 'pad_token': '<pad>', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"THUDM/chatglm-6b\", trust_remote_code=True)\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('chatglm_tokenizer\\\\tokenizer_config.json',\n",
       " 'chatglm_tokenizer\\\\special_tokens_map.json',\n",
       " 'chatglm_tokenizer\\\\ice_text.model',\n",
       " 'chatglm_tokenizer\\\\added_tokens.json')"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained(\"chatglm_tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"chatglm_tokenizer\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'弱小的我也有大Dreaming!'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenizer.encode(sen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
