{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizer 基本使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 学术资源加速\n",
    "import subprocess\n",
    "import os\n",
    "\n",
    "result = subprocess.run('bash -c \"source /etc/network_turbo && env | grep proxy\"', shell=True, capture_output=True, text=True)\n",
    "output = result.stdout\n",
    "for line in output.splitlines():\n",
    "    if '=' in line:\n",
    "        var, value = line.split('=', 1)\n",
    "        os.environ[var] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20b710ddd0e1404e8bf0c176e7e3749c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e08b9f68179a40cc918c3bacdc058040",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "409cd77565f94f7da8f9902d2d07805b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9eb4a4e1add40b5b8e849c461d66cd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# 加载预训练的Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# 待处理的文本\n",
    "text = \"Transformers are the core of modern NLP tasks.\"\n",
    "\n",
    "# 使用Tokenizer进行编码\n",
    "encoded_input = tokenizer(text, return_tensors='pt')\n",
    "\n",
    "# 访问编码结果\n",
    "input_ids = encoded_input['input_ids']\n",
    "attention_mask = encoded_input['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sen = \"吃葡萄不吐葡萄皮!\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step1 加载与保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertTokenizerFast(name_or_path='uer/roberta-base-finetuned-dianping-chinese', vocab_size=21128, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 从HuggingFace加载，输入模型名称，即可加载对于的分词器\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"uer/roberta-base-finetuned-dianping-chinese\")\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./roberta_tokenizer/tokenizer_config.json',\n",
       " './roberta_tokenizer/special_tokens_map.json',\n",
       " './roberta_tokenizer/vocab.txt',\n",
       " './roberta_tokenizer/added_tokens.json',\n",
       " './roberta_tokenizer/tokenizer.json')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenizer 保存到本地\n",
    "tokenizer.save_pretrained(\"./roberta_tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertTokenizerFast(name_or_path='./roberta_tokenizer/', vocab_size=21128, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 从本地加载tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./roberta_tokenizer/\")\n",
    "tokenizer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step2 句子分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['吃', '葡', '萄', '不', '吐', '葡', '萄', '皮', '!']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = tokenizer.tokenize(sen)\n",
    "tokens"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step3 查看词典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'##叉': 14406,\n",
       " '##蘆': 19035,\n",
       " '吟': 1412,\n",
       " '##煌': 17259,\n",
       " '手': 2797,\n",
       " 'に': 558,\n",
       " '##肋': 18547,\n",
       " '話': 6282,\n",
       " '##爆': 17312,\n",
       " 'by': 8120,\n",
       " '##eep': 12293,\n",
       " 'm1': 10211,\n",
       " '霜': 7458,\n",
       " 'time': 8759,\n",
       " '輾': 6747,\n",
       " '##怅': 15637,\n",
       " '##篡': 18127,\n",
       " 'neo': 12169,\n",
       " '##莽': 18875,\n",
       " 'ieee': 12272,\n",
       " '贲': 6584,\n",
       " '##噢': 14745,\n",
       " '##轩': 19816,\n",
       " '##嗖': 14683,\n",
       " '倍': 945,\n",
       " '##shi': 9655,\n",
       " '##media': 10970,\n",
       " 'gtx': 11069,\n",
       " '##沂': 16809,\n",
       " '拢': 2879,\n",
       " '脘': 5557,\n",
       " '侄': 888,\n",
       " '##扉': 15853,\n",
       " '刃': 1145,\n",
       " '##ᄆ': 13459,\n",
       " '＾': 8048,\n",
       " '##擘': 16143,\n",
       " '##魅': 20848,\n",
       " '##晝': 16299,\n",
       " '⌒': 404,\n",
       " '##bo': 8820,\n",
       " '##邨': 19988,\n",
       " '##雅': 20471,\n",
       " 'notes': 13165,\n",
       " '踟': 6676,\n",
       " '##艺': 18743,\n",
       " '##捻': 16008,\n",
       " '##绩': 18384,\n",
       " '亞': 765,\n",
       " '543': 11895,\n",
       " 'eq': 11601,\n",
       " 'pure': 13179,\n",
       " '掺': 2982,\n",
       " '##聖': 18526,\n",
       " '##爺': 17327,\n",
       " '##．': 21082,\n",
       " '←': 368,\n",
       " '蜱': 6061,\n",
       " '肄': 5485,\n",
       " '留': 4522,\n",
       " 'member': 11120,\n",
       " '##men': 11839,\n",
       " '##值': 14023,\n",
       " '##椰': 16553,\n",
       " '肴': 5510,\n",
       " '綏': 5193,\n",
       " '痈': 4569,\n",
       " '偈': 970,\n",
       " '##偵': 14037,\n",
       " '0t': 10691,\n",
       " '##絵': 18246,\n",
       " '籤': 5097,\n",
       " '##lot': 10580,\n",
       " '卧': 1309,\n",
       " '##劳': 14284,\n",
       " '##剽': 14260,\n",
       " '##天': 14978,\n",
       " '蜚': 6056,\n",
       " '##窩': 18036,\n",
       " '##エ': 13682,\n",
       " '噗': 1684,\n",
       " '##缙': 18415,\n",
       " 'ㆍ': 666,\n",
       " '##艾': 18744,\n",
       " '擘': 3086,\n",
       " '##訊': 19301,\n",
       " '菇': 5823,\n",
       " '##撻': 16128,\n",
       " '##緬': 18285,\n",
       " '##哄': 14560,\n",
       " '讪': 6376,\n",
       " '##臆': 18678,\n",
       " '鲸': 7841,\n",
       " '##斡': 16221,\n",
       " '##ace': 10026,\n",
       " '正': 3633,\n",
       " '吻': 1431,\n",
       " '##址': 14827,\n",
       " '##灯': 17185,\n",
       " '##痴': 17647,\n",
       " '凱': 1134,\n",
       " '語': 6295,\n",
       " '##粮': 18174,\n",
       " '##脫': 18619,\n",
       " '##话': 19470,\n",
       " '##陌': 20416,\n",
       " '魷': 7799,\n",
       " 'wang': 9660,\n",
       " '詰': 6281,\n",
       " '椎': 3491,\n",
       " '##砌': 17831,\n",
       " '##負': 19568,\n",
       " '##巷': 15407,\n",
       " '##sp': 10367,\n",
       " '縝': 5237,\n",
       " '評': 6268,\n",
       " 'news': 8501,\n",
       " '灬': 4126,\n",
       " '##缮': 18427,\n",
       " '品': 1501,\n",
       " '簇': 5077,\n",
       " '踪': 6679,\n",
       " '盖': 4667,\n",
       " '##叢': 14422,\n",
       " '##疟': 17606,\n",
       " '##檳': 16656,\n",
       " '歇': 3623,\n",
       " '112': 9017,\n",
       " '##丼': 13770,\n",
       " '挠': 2912,\n",
       " '空': 4958,\n",
       " '舟': 5660,\n",
       " '貝': 6509,\n",
       " '贫': 6577,\n",
       " '349': 12110,\n",
       " '##ゆ': 13676,\n",
       " 'rights': 9615,\n",
       " '##漲': 17096,\n",
       " '裱': 6177,\n",
       " '∮': 387,\n",
       " '##电': 17567,\n",
       " '攙': 3107,\n",
       " '##矯': 17823,\n",
       " '##袪': 19214,\n",
       " '##陵': 20434,\n",
       " '馮': 7681,\n",
       " '鸟': 7881,\n",
       " 'apr': 9011,\n",
       " 'idc': 9617,\n",
       " '##icon': 12829,\n",
       " '##ل': 13435,\n",
       " '##錚': 20147,\n",
       " '##鬥': 20841,\n",
       " '濛': 4088,\n",
       " 'special': 9969,\n",
       " '##賑': 19596,\n",
       " '嗚': 1627,\n",
       " '麝': 7928,\n",
       " '这': 6821,\n",
       " '蟠': 6098,\n",
       " '##乎': 13782,\n",
       " '籍': 5093,\n",
       " 'woshipm': 9712,\n",
       " '##ㄟ': 13718,\n",
       " '##淅': 16954,\n",
       " '蝠': 6075,\n",
       " '##なり': 11592,\n",
       " '痂': 4566,\n",
       " '2020': 8439,\n",
       " '扫': 2812,\n",
       " '虎': 5988,\n",
       " '##冯': 14158,\n",
       " '##番': 17585,\n",
       " '##悱': 15706,\n",
       " '##蓼': 18967,\n",
       " '##鍥': 20162,\n",
       " '炮': 4152,\n",
       " '##ction': 9116,\n",
       " '驢': 7714,\n",
       " '##佟': 13928,\n",
       " '炀': 4137,\n",
       " '##举': 13772,\n",
       " '籠': 5096,\n",
       " '##妃': 15021,\n",
       " '誹': 6308,\n",
       " '##ors': 10903,\n",
       " '##汛': 16790,\n",
       " '##赅': 19656,\n",
       " 'wap': 9530,\n",
       " '##锹': 20303,\n",
       " '##70': 8740,\n",
       " '##猜': 17396,\n",
       " '娩': 2030,\n",
       " 'lte': 8987,\n",
       " '##home': 10479,\n",
       " '～': 8080,\n",
       " '6500': 11247,\n",
       " '2a': 11646,\n",
       " 'self': 12178,\n",
       " 'eps': 9629,\n",
       " '3800': 11433,\n",
       " '[unused65]': 65,\n",
       " '閑': 7277,\n",
       " '##kw': 10446,\n",
       " '蕪': 5940,\n",
       " 'cafe2017': 8986,\n",
       " '##帑': 15420,\n",
       " '##詳': 19341,\n",
       " '##鎊': 20168,\n",
       " '##よ': 9427,\n",
       " '925': 10711,\n",
       " '艇': 5673,\n",
       " '閒': 7278,\n",
       " '匯': 1274,\n",
       " '##岱': 15333,\n",
       " '##濡': 17148,\n",
       " '霖': 7457,\n",
       " '##午': 14343,\n",
       " '毗': 3685,\n",
       " '栃': 3400,\n",
       " '僑': 1009,\n",
       " '8cm': 12237,\n",
       " '轟': 6755,\n",
       " '韵': 7510,\n",
       " '呢': 1450,\n",
       " '贺': 6590,\n",
       " '埔': 1815,\n",
       " '##柵': 16453,\n",
       " '那': 6929,\n",
       " '383': 12510,\n",
       " '聲': 5476,\n",
       " '##dget': 11857,\n",
       " 'atm': 8366,\n",
       " '##梆': 16507,\n",
       " '籟': 5095,\n",
       " '蜴': 6062,\n",
       " 'dear': 11694,\n",
       " '##相': 17742,\n",
       " '##寥': 15235,\n",
       " '##镜': 20319,\n",
       " '芫': 5704,\n",
       " '413': 12561,\n",
       " '骗': 7745,\n",
       " '26': 8153,\n",
       " '##cord': 12291,\n",
       " '飕': 7602,\n",
       " '##祁': 17911,\n",
       " '##澱': 17134,\n",
       " '昇': 3205,\n",
       " 'cio': 9351,\n",
       " '##玖': 17433,\n",
       " '##呋': 14498,\n",
       " '##牛': 17338,\n",
       " '##蜇': 19104,\n",
       " '##紳': 18227,\n",
       " 'fedora': 11577,\n",
       " '贈': 6557,\n",
       " '##艘': 18732,\n",
       " '##痘': 17633,\n",
       " 'yy': 9453,\n",
       " '##匱': 14332,\n",
       " 'tai': 13242,\n",
       " '##濛': 17145,\n",
       " '%': 110,\n",
       " '澧': 4076,\n",
       " '紧': 5165,\n",
       " '##俑': 13977,\n",
       " '##郭': 20015,\n",
       " '##虽': 19063,\n",
       " '##爷': 17324,\n",
       " '##free': 13287,\n",
       " '翻': 5436,\n",
       " '咁': 1464,\n",
       " '##ｃ': 10675,\n",
       " 'today': 11262,\n",
       " '##ᵃ': 13489,\n",
       " 'bot': 11825,\n",
       " '##於': 16233,\n",
       " 'april': 9557,\n",
       " '##晏': 16291,\n",
       " '##耸': 18515,\n",
       " '##蟻': 19159,\n",
       " 'ram': 9270,\n",
       " '届': 2237,\n",
       " '##搬': 16078,\n",
       " '##芦': 18758,\n",
       " '##選': 19965,\n",
       " '##劣': 14276,\n",
       " '019': 13146,\n",
       " 'さ': 546,\n",
       " '黜': 7951,\n",
       " 'ᅴ': 319,\n",
       " '##侄': 13945,\n",
       " '汕': 3730,\n",
       " '##急': 15650,\n",
       " '##me': 8505,\n",
       " '##的': 17695,\n",
       " '625': 12218,\n",
       " 'miacare': 11918,\n",
       " '##脑': 18611,\n",
       " '##恪': 15675,\n",
       " '铐': 7196,\n",
       " '1968': 9045,\n",
       " '袒': 6152,\n",
       " '##鳍': 20903,\n",
       " '丛': 690,\n",
       " '##拢': 15936,\n",
       " '##汀': 16779,\n",
       " '震': 7448,\n",
       " '##ぇ': 13669,\n",
       " '##滬': 17071,\n",
       " 'dm': 10442,\n",
       " '眙': 4694,\n",
       " '乘': 733,\n",
       " '##允': 14095,\n",
       " '晚': 3241,\n",
       " '遇': 6878,\n",
       " '燉': 4237,\n",
       " 'body': 9867,\n",
       " '泫': 3802,\n",
       " 'ﾗ': 8095,\n",
       " '##ein': 11858,\n",
       " 'boost': 11986,\n",
       " '##dge': 12025,\n",
       " '##tive': 9741,\n",
       " 'north': 10744,\n",
       " '5c': 13025,\n",
       " '桥': 3441,\n",
       " '##冀': 14135,\n",
       " '##ias': 12803,\n",
       " '同': 1398,\n",
       " '##划': 14210,\n",
       " '##渔': 16991,\n",
       " '绮': 5331,\n",
       " '##略': 17583,\n",
       " '呈': 1439,\n",
       " '授': 2956,\n",
       " 'mama': 13186,\n",
       " '準': 3976,\n",
       " '齣': 7974,\n",
       " 'frm': 11794,\n",
       " '##lter': 12171,\n",
       " '##ᆷ': 13484,\n",
       " 'tcp': 9901,\n",
       " 'ssd': 9004,\n",
       " '##当': 15553,\n",
       " '##畜': 17580,\n",
       " '層': 2251,\n",
       " '##美': 18458,\n",
       " '##灶': 17188,\n",
       " '最': 3297,\n",
       " '租': 4909,\n",
       " '##贱': 19640,\n",
       " '测': 3844,\n",
       " '»': 188,\n",
       " '傥': 995,\n",
       " '鳝': 7850,\n",
       " '##妳': 15043,\n",
       " '##λ': 13389,\n",
       " 'cell': 11490,\n",
       " '##筍': 18083,\n",
       " '##ly': 8436,\n",
       " '##耨': 18511,\n",
       " '##評': 19325,\n",
       " '##邢': 19985,\n",
       " '##骆': 20793,\n",
       " '##衄': 19175,\n",
       " '琅': 4414,\n",
       " '灶': 4131,\n",
       " '雎': 7421,\n",
       " '##骸': 20817,\n",
       " '##mate': 12125,\n",
       " '##ｋ': 10981,\n",
       " '250': 8401,\n",
       " '##屋': 15295,\n",
       " '媚': 2055,\n",
       " '碴': 4824,\n",
       " '耕': 5449,\n",
       " '脱': 5564,\n",
       " '馀': 7663,\n",
       " 'ง': 277,\n",
       " '##ways': 11672,\n",
       " '##扯': 15873,\n",
       " '##拜': 15933,\n",
       " 'cn': 8274,\n",
       " '##槛': 16603,\n",
       " '蒿': 5896,\n",
       " 'ｲ': 8088,\n",
       " '##摊': 16090,\n",
       " '##突': 18017,\n",
       " '##﹞': 21067,\n",
       " '##ンク': 9262,\n",
       " 'mtv': 11529,\n",
       " '##◆': 10497,\n",
       " '##700': 11551,\n",
       " '##佚': 13923,\n",
       " 'pt': 10791,\n",
       " 'vga': 13140,\n",
       " '鹈': 7901,\n",
       " '些': 763,\n",
       " '[unused82]': 82,\n",
       " '##は': 8319,\n",
       " '##lli': 13169,\n",
       " '##抚': 15893,\n",
       " 'ktv': 8894,\n",
       " '##ration': 11103,\n",
       " '潮': 4060,\n",
       " '相': 4685,\n",
       " 'inparadise': 12484,\n",
       " '四': 1724,\n",
       " '##莒': 18859,\n",
       " '滞': 4005,\n",
       " '侖': 895,\n",
       " '##澄': 17124,\n",
       " 'l1': 12421,\n",
       " 'cool1': 12032,\n",
       " '赌': 6603,\n",
       " '##彌': 15550,\n",
       " '滄': 3993,\n",
       " '哩': 1524,\n",
       " '徐': 2528,\n",
       " 'pony': 8365,\n",
       " '##醫': 20072,\n",
       " 'ㄆ': 648,\n",
       " '##けと': 10095,\n",
       " '##肓': 18550,\n",
       " '##舵': 18723,\n",
       " '妣': 1978,\n",
       " '##int': 9824,\n",
       " '##阀': 20379,\n",
       " '憑': 2731,\n",
       " '窨': 4978,\n",
       " '##惠': 15726,\n",
       " '##漢': 17088,\n",
       " '娛': 2024,\n",
       " '抄': 2826,\n",
       " '##cr': 13129,\n",
       " '##锺': 20304,\n",
       " '##哥': 14577,\n",
       " '绥': 5324,\n",
       " '其': 1071,\n",
       " '缈': 5350,\n",
       " '廖': 2445,\n",
       " '儿': 1036,\n",
       " '渙': 3936,\n",
       " '鯽': 7811,\n",
       " '催': 998,\n",
       " '兔': 1052,\n",
       " '##卡': 14362,\n",
       " '铸': 7214,\n",
       " '応': 2565,\n",
       " '邂': 6915,\n",
       " '逞': 6861,\n",
       " '##俏': 13975,\n",
       " '妹': 1987,\n",
       " '栏': 3408,\n",
       " '##姚': 15058,\n",
       " '恐': 2607,\n",
       " '##ˍ': 13375,\n",
       " '##頂': 20572,\n",
       " '##蛾': 19099,\n",
       " '又': 1348,\n",
       " 'м': 244,\n",
       " '叙': 1360,\n",
       " '歷': 3644,\n",
       " 'ﾙ': 8096,\n",
       " 'work': 10782,\n",
       " '粿': 5126,\n",
       " '筊': 5024,\n",
       " 'watch': 9114,\n",
       " 'jquery': 10035,\n",
       " '铜': 7198,\n",
       " '##ose': 10936,\n",
       " '尻': 2224,\n",
       " '溱': 3986,\n",
       " '頜': 7527,\n",
       " '##岘': 15324,\n",
       " '弓': 2469,\n",
       " '焜': 4191,\n",
       " '##man': 8490,\n",
       " '##hn': 11781,\n",
       " '##復': 15598,\n",
       " '##憨': 15793,\n",
       " '##棠': 16535,\n",
       " '##濑': 17141,\n",
       " '諷': 6327,\n",
       " '##眼': 17763,\n",
       " '##耕': 18506,\n",
       " '1953': 9268,\n",
       " '飼': 7615,\n",
       " '##裕': 19225,\n",
       " '２０': 10327,\n",
       " '肺': 5511,\n",
       " 'google': 8190,\n",
       " '迢': 6827,\n",
       " '##tes': 9420,\n",
       " '##鄉': 20022,\n",
       " '录': 2497,\n",
       " '##妓': 15029,\n",
       " '瑕': 4442,\n",
       " '##帽': 15441,\n",
       " '时': 3198,\n",
       " '##惚': 15723,\n",
       " '##bs': 9071,\n",
       " '58': 8255,\n",
       " '##う': 8981,\n",
       " '##或': 15829,\n",
       " 'alexander': 11733,\n",
       " '##恐': 15664,\n",
       " '##贓': 19618,\n",
       " '##26': 8756,\n",
       " '##をこ': 12335,\n",
       " '##襬': 19259,\n",
       " '羈': 5398,\n",
       " '冲': 1103,\n",
       " '袋': 6150,\n",
       " '讷': 6386,\n",
       " '酝': 6986,\n",
       " '柚': 3384,\n",
       " '##您': 15701,\n",
       " '##涸': 16949,\n",
       " 'マ': 624,\n",
       " '##慰': 15777,\n",
       " '##滅': 17051,\n",
       " '##猝': 17397,\n",
       " '釦': 7038,\n",
       " '##债': 14022,\n",
       " '##搂': 16065,\n",
       " '─': 427,\n",
       " '鑊': 7140,\n",
       " '婪': 2046,\n",
       " '｜': 8078,\n",
       " '##イト': 10516,\n",
       " '眨': 4699,\n",
       " 'loading': 10878,\n",
       " '##缸': 18431,\n",
       " '泠': 3795,\n",
       " '##诏': 19462,\n",
       " '##踌': 19728,\n",
       " '##閏': 20333,\n",
       " '培': 1824,\n",
       " '##燜': 17301,\n",
       " '3500': 9252,\n",
       " '句': 1368,\n",
       " '煽': 4218,\n",
       " '##全': 14116,\n",
       " '20': 8113,\n",
       " '##age': 9103,\n",
       " '##渴': 17008,\n",
       " '##蕁': 18987,\n",
       " '##鍵': 20164,\n",
       " '##胤': 18587,\n",
       " '芎': 5694,\n",
       " '##卒': 14350,\n",
       " 'elle': 11593,\n",
       " '##摹': 16101,\n",
       " '（': 8020,\n",
       " 'title': 9956,\n",
       " '##疮': 17612,\n",
       " '##秃': 17958,\n",
       " 'jpeg': 11985,\n",
       " '窜': 4972,\n",
       " '##貢': 19570,\n",
       " '##隅': 20440,\n",
       " '效': 3126,\n",
       " '##宴': 15212,\n",
       " '161': 9608,\n",
       " '##.': 13330,\n",
       " '姗': 2000,\n",
       " 'css': 8935,\n",
       " '稠': 4932,\n",
       " 'program': 11738,\n",
       " '##厘': 14387,\n",
       " '##衲': 19197,\n",
       " '冇': 1081,\n",
       " '##dom': 11290,\n",
       " '－': 8025,\n",
       " '##臥': 18686,\n",
       " 'claire': 9753,\n",
       " '##蜴': 19119,\n",
       " '±': 181,\n",
       " '烃': 4163,\n",
       " '##l': 8178,\n",
       " '谟': 6467,\n",
       " '籮': 5099,\n",
       " '12345678910': 9363,\n",
       " '喉': 1590,\n",
       " '##蹴': 19760,\n",
       " '捎': 2933,\n",
       " '岭': 2275,\n",
       " '泽': 3813,\n",
       " '##∽': 13541,\n",
       " '徴': 2546,\n",
       " '铢': 7202,\n",
       " '##哔': 14573,\n",
       " 'ー': 645,\n",
       " 'lost': 12593,\n",
       " '訝': 6252,\n",
       " '瘫': 4611,\n",
       " '##匐': 14320,\n",
       " '聴': 5478,\n",
       " 'ling': 10061,\n",
       " 'eyes': 12909,\n",
       " '##秘': 17965,\n",
       " '##ᅴ': 13483,\n",
       " '儲': 1033,\n",
       " 'limited': 10424,\n",
       " '##岳': 15334,\n",
       " '永': 3719,\n",
       " 'i3': 12224,\n",
       " '筠': 5035,\n",
       " '嘜': 1659,\n",
       " '##cky': 11216,\n",
       " '識': 6352,\n",
       " '賢': 6545,\n",
       " '##今': 13848,\n",
       " 'oem': 9945,\n",
       " '3000': 8283,\n",
       " 'vs': 8349,\n",
       " '##谢': 19525,\n",
       " '倬': 962,\n",
       " '护': 2844,\n",
       " '秃': 4901,\n",
       " '[unused30]': 30,\n",
       " '##煞': 17265,\n",
       " '106': 8438,\n",
       " '##と': 8322,\n",
       " 'av': 8375,\n",
       " '##孜': 15161,\n",
       " '賊': 6538,\n",
       " 'ᅦ': 308,\n",
       " '##朱': 16376,\n",
       " '##迈': 19872,\n",
       " '##瓶': 17543,\n",
       " '##aka': 12417,\n",
       " '##喆': 14645,\n",
       " '彗': 2498,\n",
       " '妳': 1986,\n",
       " '##□': 13606,\n",
       " '##how': 13132,\n",
       " '喬': 1605,\n",
       " '稜': 4929,\n",
       " '##蜘': 19112,\n",
       " '↓': 371,\n",
       " 'single': 12148,\n",
       " '螳': 6089,\n",
       " '##tory': 12608,\n",
       " '##饍': 20696,\n",
       " '黛': 7950,\n",
       " '仲': 815,\n",
       " '缕': 5355,\n",
       " '遠': 6895,\n",
       " '1990': 8431,\n",
       " '232': 9863,\n",
       " '沖': 3762,\n",
       " '##梁': 16505,\n",
       " '##焦': 17250,\n",
       " '\\\\': 139,\n",
       " '##嵬': 15377,\n",
       " '繽': 5263,\n",
       " '##eme': 12538,\n",
       " '##屿': 15314,\n",
       " '##霎': 20510,\n",
       " '屁': 2230,\n",
       " '##栉': 16462,\n",
       " '##湘': 17017,\n",
       " 'yumi': 11697,\n",
       " 'weeks': 11973,\n",
       " '##劭': 14281,\n",
       " 'xp': 8766,\n",
       " '粳': 5120,\n",
       " '##net': 8914,\n",
       " 'ide': 11319,\n",
       " '##盲': 17740,\n",
       " '##丶': 13765,\n",
       " '普': 3249,\n",
       " '##喋': 14649,\n",
       " '颱': 7593,\n",
       " '##溺': 17046,\n",
       " '##駁': 20741,\n",
       " '脛': 5559,\n",
       " '##卍': 14346,\n",
       " '##73': 9148,\n",
       " '##罔': 18439,\n",
       " '▌♥': 9601,\n",
       " '胁': 5516,\n",
       " '##蚁': 19066,\n",
       " '##芎': 18751,\n",
       " '##窟': 18031,\n",
       " '##课': 19497,\n",
       " '##荻': 18851,\n",
       " '##繼': 18319,\n",
       " '##鎏': 20170,\n",
       " '減': 3938,\n",
       " '売': 1899,\n",
       " 'し': 547,\n",
       " '湊': 3957,\n",
       " '访': 6393,\n",
       " '##モ': 13694,\n",
       " 'ff': 12388,\n",
       " '##x2': 12505,\n",
       " '##专': 13740,\n",
       " '##棲': 16540,\n",
       " '##貧': 19571,\n",
       " '衢': 6131,\n",
       " '##锑': 20287,\n",
       " 'database': 12435,\n",
       " '啧': 1569,\n",
       " '央': 1925,\n",
       " 'good': 9005,\n",
       " 'canada': 12947,\n",
       " '##畅': 17574,\n",
       " '扭': 2814,\n",
       " '##芮': 18764,\n",
       " '##鸣': 20942,\n",
       " '駅': 7686,\n",
       " '檢': 3596,\n",
       " '##c': 8177,\n",
       " '瀋': 4103,\n",
       " '34': 8229,\n",
       " 'may': 8480,\n",
       " '##她': 15018,\n",
       " '##拾': 15953,\n",
       " '##eng': 9995,\n",
       " '##芸': 18769,\n",
       " '##刊': 14206,\n",
       " '##读': 19495,\n",
       " '俐': 919,\n",
       " '##趵': 19698,\n",
       " '俯': 935,\n",
       " '啻': 1581,\n",
       " '##dden': 13109,\n",
       " '##雞': 20487,\n",
       " '##闊': 20352,\n",
       " '已': 2347,\n",
       " '厘': 1330,\n",
       " '忻': 2574,\n",
       " 'ａ': 8051,\n",
       " 'sense': 12647,\n",
       " '##忪': 15627,\n",
       " '參': 1347,\n",
       " 'pixnet': 8191,\n",
       " '##cil': 11783,\n",
       " '犍': 4301,\n",
       " '##罹': 18452,\n",
       " '##point': 11112,\n",
       " '##直': 17741,\n",
       " '斗': 3159,\n",
       " '##鷗': 20932,\n",
       " '携': 3025,\n",
       " '稹': 4939,\n",
       " '巽': 2352,\n",
       " 'nata': 10166,\n",
       " 'spf': 12598,\n",
       " '瘀': 4595,\n",
       " 'ه': 272,\n",
       " '##祖': 17919,\n",
       " '设': 6392,\n",
       " '##ほ': 13672,\n",
       " '侍': 892,\n",
       " '樞': 3561,\n",
       " '湳': 3967,\n",
       " '##煊': 17258,\n",
       " '##巫': 15401,\n",
       " '眺': 4705,\n",
       " '諧': 6321,\n",
       " '##坍': 14831,\n",
       " '##瑄': 17497,\n",
       " '##cs': 9761,\n",
       " '筑': 5029,\n",
       " '##旭': 16252,\n",
       " '鞍': 7491,\n",
       " '柘': 3383,\n",
       " '##シャンルの': 11947,\n",
       " '##ことか': 10274,\n",
       " '##承': 15881,\n",
       " '虽': 6006,\n",
       " '##娶': 15091,\n",
       " '蔽': 5929,\n",
       " '闆': 7293,\n",
       " '##啰': 14631,\n",
       " '##烨': 17231,\n",
       " '##紀': 18202,\n",
       " '傻': 1004,\n",
       " '擎': 3083,\n",
       " '枭': 3368,\n",
       " '皙': 4647,\n",
       " '胫': 5533,\n",
       " '366': 11949,\n",
       " '##ons': 9961,\n",
       " '##ets': 12198,\n",
       " '##耽': 18517,\n",
       " 'r9': 12674,\n",
       " '##sy': 10178,\n",
       " '##cle': 11619,\n",
       " '##ssion': 12726,\n",
       " '##處': 19050,\n",
       " '盞': 4672,\n",
       " '坚': 1780,\n",
       " '##賽': 19612,\n",
       " '##h': 8199,\n",
       " '凝': 1125,\n",
       " '##距': 19712,\n",
       " 'shop': 9926,\n",
       " '玩': 4381,\n",
       " '##カー': 10175,\n",
       " '##兄': 14097,\n",
       " '倭': 963,\n",
       " '##泾': 16871,\n",
       " '擱': 3095,\n",
       " '5757': 12007,\n",
       " 'cisco': 11010,\n",
       " '##贿': 19651,\n",
       " '##內': 14115,\n",
       " '吨': 1417,\n",
       " '哉': 1507,\n",
       " '衮': 6138,\n",
       " 'food': 9579,\n",
       " '燁': 4233,\n",
       " '##ᆸ': 13485,\n",
       " '格': 3419,\n",
       " '##フト': 10868,\n",
       " '##净': 14169,\n",
       " '##審': 15239,\n",
       " '##懶': 15811,\n",
       " '##抛': 15894,\n",
       " '##疚': 17604,\n",
       " '##腕': 18637,\n",
       " '##館': 20688,\n",
       " '蘭': 5984,\n",
       " '淨': 3912,\n",
       " '##ric': 11748,\n",
       " '##榨': 16586,\n",
       " '惆': 2659,\n",
       " '309b': 9799,\n",
       " '##やり': 11077,\n",
       " 'su': 11541,\n",
       " '##轲': 19822,\n",
       " '##感': 15754,\n",
       " '##醒': 20065,\n",
       " '†': 339,\n",
       " '砂': 4773,\n",
       " '视': 6228,\n",
       " '鎚': 7117,\n",
       " '##ford': 10283,\n",
       " 'bbe': 12517,\n",
       " '##輕': 19795,\n",
       " '##絡': 18238,\n",
       " '塾': 1860,\n",
       " 'nike': 8702,\n",
       " '缢': 5363,\n",
       " '野': 7029,\n",
       " '##ova': 13067,\n",
       " '铝': 7199,\n",
       " '##mi': 8625,\n",
       " '##憋': 15785,\n",
       " '##掲': 16034,\n",
       " '##恁': 15659,\n",
       " '1983': 8715,\n",
       " '##摈': 16089,\n",
       " 'ᆸ': 325,\n",
       " '椁': 3487,\n",
       " '谌': 6451,\n",
       " '＜': 8040,\n",
       " '貽': 6529,\n",
       " '##ua': 9478,\n",
       " '##da': 8521,\n",
       " '##han': 9646,\n",
       " '🔥': 8103,\n",
       " 'fit': 11188,\n",
       " '孱': 2116,\n",
       " '##ess': 9685,\n",
       " '##ᅮ': 13479,\n",
       " '##ew': 10795,\n",
       " '##咚': 14537,\n",
       " 'helpapp': 10609,\n",
       " '溏': 3974,\n",
       " '##姊': 15049,\n",
       " '71': 8459,\n",
       " 'с': 249,\n",
       " '##楨': 16566,\n",
       " 'cf1': 12327,\n",
       " '##顾': 20617,\n",
       " '骞': 7749,\n",
       " 'thai': 12967,\n",
       " '⑴': 415,\n",
       " 'はい': 9781,\n",
       " '##盱': 17739,\n",
       " '##yo': 10222,\n",
       " '犷': 4308,\n",
       " '诸': 6436,\n",
       " '##釵': 20097,\n",
       " '##豐': 19550,\n",
       " '他': 800,\n",
       " '✪': 502,\n",
       " '痔': 4574,\n",
       " '虑': 5991,\n",
       " '##周': 14510,\n",
       " '##澎': 17128,\n",
       " '##缔': 18411,\n",
       " '短': 4764,\n",
       " '<T>': 105,\n",
       " 'source': 9014,\n",
       " '##琼': 17494,\n",
       " '赖': 6609,\n",
       " '##nk': 9705,\n",
       " '223': 10265,\n",
       " '##●': 9037,\n",
       " '##襁': 19254,\n",
       " '##〔': 13661,\n",
       " '##蛆': 19082,\n",
       " '##锏': 20285,\n",
       " '跳': 6663,\n",
       " '##▉': 13601,\n",
       " '揽': 3005,\n",
       " 'x5': 10871,\n",
       " '仇': 790,\n",
       " 'jayz': 11072,\n",
       " '52sykb': 12846,\n",
       " '##瓜': 17535,\n",
       " '##布': 15414,\n",
       " '毙': 3687,\n",
       " 'more': 8384,\n",
       " '6s': 9590,\n",
       " '##ston': 10229,\n",
       " '374': 11948,\n",
       " '##彈': 15549,\n",
       " '##響': 20570,\n",
       " '睽': 4727,\n",
       " '##嫑': 15124,\n",
       " '闸': 7316,\n",
       " '##泓': 16847,\n",
       " 'f1': 9080,\n",
       " '##秽': 17977,\n",
       " '書': 3292,\n",
       " '俱': 936,\n",
       " '萵': 5859,\n",
       " '##圧': 14818,\n",
       " '晤': 3245,\n",
       " '總': 5244,\n",
       " '##島': 15351,\n",
       " '晟': 3244,\n",
       " '##瞅': 17788,\n",
       " '##ハー': 10977,\n",
       " '144': 9494,\n",
       " '奶': 1959,\n",
       " '##阈': 20385,\n",
       " '##阻': 20406,\n",
       " '##③': 13558,\n",
       " '把': 2828,\n",
       " '馁': 7664,\n",
       " 'amoled': 11307,\n",
       " '骷': 7759,\n",
       " 'tripadvisor': 8194,\n",
       " '牌': 4277,\n",
       " '##ted': 9255,\n",
       " '##竽': 18060,\n",
       " '##縛': 18293,\n",
       " '##黄': 20999,\n",
       " '##憫': 15795,\n",
       " '##mmy': 12137,\n",
       " '汰': 3743,\n",
       " '790': 11793,\n",
       " '宓': 2132,\n",
       " '##ヘ': 13693,\n",
       " '##链': 20273,\n",
       " '##哂': 14559,\n",
       " '##烟': 17227,\n",
       " '咀': 1463,\n",
       " '##缜': 18417,\n",
       " '釁': 7022,\n",
       " '欖': 3611,\n",
       " '祯': 4875,\n",
       " '逕': 6855,\n",
       " '素': 5162,\n",
       " '醋': 7005,\n",
       " 'crystal': 12070,\n",
       " '环': 4384,\n",
       " '茲': 5760,\n",
       " '626': 12463,\n",
       " 'but': 10288,\n",
       " '##喧': 14659,\n",
       " '赫': 6622,\n",
       " '费': 6589,\n",
       " '▲topsep': 10625,\n",
       " '##耳': 18512,\n",
       " '364': 12673,\n",
       " '〉': 516,\n",
       " '##泄': 16843,\n",
       " '騙': 7700,\n",
       " '典': 1073,\n",
       " '##码': 17829,\n",
       " '卢': 1306,\n",
       " '弟': 2475,\n",
       " '膊': 5600,\n",
       " '##帐': 15419,\n",
       " '垃': 1796,\n",
       " '##洙': 16877,\n",
       " '##娃': 15072,\n",
       " 'test': 10060,\n",
       " ...}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21128"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab_size"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step4 索引转换"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1391, 5868, 5843, 679, 1402, 5868, 5843, 4649, 106]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 将词序列转换为id序列\n",
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['吃', '葡', '萄', '不', '吐', '葡', '萄', '皮', '!']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 将id序列转换为token序列\n",
    "tokens = tokenizer.convert_ids_to_tokens(ids)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'吃 葡 萄 不 吐 葡 萄 皮!'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 将token序列转换为string\n",
    "str_sen = tokenizer.convert_tokens_to_string(tokens)\n",
    "str_sen"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  更便捷的实现方式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 1391, 5868, 5843, 679, 1402, 5868, 5843, 4649, 106, 102]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 将字符串转换为id序列，又称之为编码\n",
    "ids = tokenizer.encode(sen, add_special_tokens=True)\n",
    "ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] 吃 葡 萄 不 吐 葡 萄 皮! [SEP]'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 将id序列转换为字符串，又称之为解码\n",
    "str_sen = tokenizer.decode(ids, skip_special_tokens=False)\n",
    "str_sen"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step5 填充与截断"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 1391, 5868, 5843, 679, 1402, 5868, 5843, 4649, 106, 102, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 填充\n",
    "ids = tokenizer.encode(sen, padding=\"max_length\", max_length=15)\n",
    "ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 1391, 5868, 5843, 102]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 截断\n",
    "ids = tokenizer.encode(sen, max_length=5, truncation=True)\n",
    "ids"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step6 其他输入部分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 1391, 5868, 5843, 679, 1402, 5868, 5843, 4649, 106, 102, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids = tokenizer.encode(sen, padding=\"max_length\", max_length=15)\n",
    "ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([101, 1391, 5868, 5843, 679, 1402, 5868, 5843, 4649, 106, 102, 0, 0, 0, 0],\n",
       " [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_mask = [1 if idx != 0 else 0 for idx in ids]\n",
    "token_type_ids = [0] * len(ids)\n",
    "ids, attention_mask, token_type_ids"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step7 快速调用方式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 1391, 5868, 5843, 679, 1402, 5868, 5843, 4649, 106, 102, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0]}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tokenizer.encode_plus(sen, padding=\"max_length\", max_length=15)\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 1391, 5868, 5843, 679, 1402, 5868, 5843, 4649, 106, 102, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0]}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tokenizer(sen, padding=\"max_length\", max_length=15)\n",
    "inputs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step8 处理batch数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[101, 1391, 5868, 5843, 679, 1402, 5868, 5843, 4649, 102], [101, 679, 1391, 5868, 5843, 1168, 1402, 5868, 5843, 4649, 102], [101, 7556, 1232, 5445, 711, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sens = [\"吃葡萄不吐葡萄皮\",\n",
    "        \"不吃葡萄到吐葡萄皮\",\n",
    "        \"顺势而为\"]\n",
    "res = tokenizer(sens)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 45.3 ms, sys: 0 ns, total: 45.3 ms\n",
      "Wall time: 44.6 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 单条循环处理\n",
    "for i in range(1000):\n",
    "    tokenizer(sen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 27.7 ms, sys: 15.6 ms, total: 43.2 ms\n",
      "Wall time: 7.68 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 处理batch数据\n",
    "res = tokenizer([sen] * 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertTokenizerFast(name_or_path='./roberta_tokenizer/', vocab_size=21128, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fast / Slow Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "sen = \"吃葡萄不吐葡萄皮!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertTokenizerFast(name_or_path='uer/roberta-base-finetuned-dianping-chinese', vocab_size=21128, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fast_tokenizer = AutoTokenizer.from_pretrained(\"uer/roberta-base-finetuned-dianping-chinese\")\n",
    "fast_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertTokenizer(name_or_path='uer/roberta-base-finetuned-dianping-chinese', vocab_size=21128, model_max_length=1000000000000000019884624838656, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slow_tokenizer = AutoTokenizer.from_pretrained(\"uer/roberta-base-finetuned-dianping-chinese\", use_fast=False)\n",
    "slow_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 443 ms, sys: 0 ns, total: 443 ms\n",
      "Wall time: 442 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 单条循环处理\n",
    "for i in range(10000):\n",
    "    fast_tokenizer(sen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.39 s, sys: 0 ns, total: 1.39 s\n",
      "Wall time: 1.39 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 单条循环处理\n",
    "for i in range(10000):\n",
    "    slow_tokenizer(sen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 323 ms, sys: 146 ms, total: 468 ms\n",
      "Wall time: 172 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 处理batch数据\n",
    "res = fast_tokenizer([sen] * 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.1 s, sys: 15.8 ms, total: 1.12 s\n",
      "Wall time: 1.12 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 处理batch数据\n",
    "res = slow_tokenizer([sen] * 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 1391, 5868, 5843, 679, 1402, 5868, 5843, 4649, 106, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'offset_mapping': [(0, 0), (0, 1), (1, 2), (2, 3), (3, 4), (4, 5), (5, 6), (6, 7), (7, 8), (8, 9), (0, 0)]}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = fast_tokenizer(sen, return_offsets_mapping=True)\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None, 0, 1, 2, 3, 4, 5, 6, 7, 8, None]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.word_ids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "return_offset_mapping is not available when using Python tokenizers. To use this feature, change your tokenizer to one deriving from transformers.PreTrainedTokenizerFast. More information on available tokenizers at https://github.com/huggingface/transformers/pull/2674",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m inputs \u001b[38;5;241m=\u001b[39m \u001b[43mslow_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43msen\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2858\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.__call__\u001b[0;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2856\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_in_target_context_manager:\n\u001b[1;32m   2857\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_input_mode()\n\u001b[0;32m-> 2858\u001b[0m     encodings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_one\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mall_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2859\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m text_target \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2860\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_target_mode()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2964\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._call_one\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2944\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_encode_plus(\n\u001b[1;32m   2945\u001b[0m         batch_text_or_text_pairs\u001b[38;5;241m=\u001b[39mbatch_text_or_text_pairs,\n\u001b[1;32m   2946\u001b[0m         add_special_tokens\u001b[38;5;241m=\u001b[39madd_special_tokens,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2961\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   2962\u001b[0m     )\n\u001b[1;32m   2963\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2964\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2965\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2966\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2967\u001b[0m \u001b[43m        \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2968\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2969\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2970\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2971\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2972\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2973\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2974\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2975\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2976\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2977\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2978\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2979\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2980\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2981\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2982\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2983\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3037\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.encode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   3027\u001b[0m \u001b[38;5;66;03m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[39;00m\n\u001b[1;32m   3028\u001b[0m padding_strategy, truncation_strategy, max_length, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_padding_truncation_strategies(\n\u001b[1;32m   3029\u001b[0m     padding\u001b[38;5;241m=\u001b[39mpadding,\n\u001b[1;32m   3030\u001b[0m     truncation\u001b[38;5;241m=\u001b[39mtruncation,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3034\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   3035\u001b[0m )\n\u001b[0;32m-> 3037\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_encode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3038\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3039\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3040\u001b[0m \u001b[43m    \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3041\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3042\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtruncation_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3043\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3044\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3045\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3046\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3047\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3048\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3049\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3050\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3051\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3052\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3053\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3054\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3055\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3056\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/transformers/tokenization_utils.py:711\u001b[0m, in \u001b[0;36mPreTrainedTokenizer._encode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    705\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    706\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not valid. Should be a string, a list/tuple of strings or a list/tuple of\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    707\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m integers.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    708\u001b[0m             )\n\u001b[1;32m    710\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_offsets_mapping:\n\u001b[0;32m--> 711\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[1;32m    712\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreturn_offset_mapping is not available when using Python tokenizers. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    713\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTo use this feature, change your tokenizer to one deriving from \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    714\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransformers.PreTrainedTokenizerFast. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    715\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMore information on available tokenizers at \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    716\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://github.com/huggingface/transformers/pull/2674\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    717\u001b[0m     )\n\u001b[1;32m    719\u001b[0m first_ids \u001b[38;5;241m=\u001b[39m get_input_ids(text)\n\u001b[1;32m    720\u001b[0m second_ids \u001b[38;5;241m=\u001b[39m get_input_ids(text_pair) \u001b[38;5;28;01mif\u001b[39;00m text_pair \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: return_offset_mapping is not available when using Python tokenizers. To use this feature, change your tokenizer to one deriving from transformers.PreTrainedTokenizerFast. More information on available tokenizers at https://github.com/huggingface/transformers/pull/2674"
     ]
    }
   ],
   "source": [
    "inputs = slow_tokenizer(sen, return_offsets_mapping=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 特殊Tokenizer的加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8357a6f05b4345baaffbb95f34fed2a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/441 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yuyao\\miniconda3\\envs\\transformers\\lib\\site-packages\\huggingface_hub\\file_download.py:133: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\yuyao\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ccfd3477c5c45e894350699ab1fffc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)enization_chatglm.py:   0%|          | 0.00/17.0k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e3ce5c1fcbb4d10b64c6b766cf99d95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading ice_text.model:   0%|          | 0.00/2.71M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "ChatGLMTokenizer(name_or_path='THUDM/chatglm-6b', vocab_size=130344, model_max_length=2048, is_fast=False, padding_side='left', truncation_side='right', special_tokens={'bos_token': '<sop>', 'eos_token': '<eop>', 'unk_token': '<unk>', 'pad_token': '<pad>', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"THUDM/chatglm-6b\", trust_remote_code=True)\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('chatglm_tokenizer\\\\tokenizer_config.json',\n",
       " 'chatglm_tokenizer\\\\special_tokens_map.json',\n",
       " 'chatglm_tokenizer\\\\ice_text.model',\n",
       " 'chatglm_tokenizer\\\\added_tokens.json')"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained(\"chatglm_tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"chatglm_tokenizer\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'弱小的我也有大Dreaming!'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenizer.encode(sen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
